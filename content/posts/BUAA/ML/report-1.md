---
title: "[BUAA-ML] 第一次实验报告"
date: 2025-04-27T21:42:13+08:00
categories: 
tags: 
author: 
showToc: true
draft: false
comments: true
description: Linear Regression
canonicalURL: "{{ .Permalink }}"
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: true
---
## 一、实验过程中，是否对输入数据进行了归一化或标准化处理？试说明这两种方法的区别，并分析为什么线性回归模型可能对特征的尺度敏感。  

### （一）实验中的预处理方法  

实验中使用了标准化处理：  

```python
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

- `fit_transform`对训练集计算均值和标准差并应用转换。  
- `transform`对测试集直接使用训练集的参数进行转换。  

### (二) 归一化与标准化的区别  

1. **归一化 (Normalization)**:  
	将数据缩放到固定范围 $[x_{\min}, x_{\max}]$，公式为：  
   
   $$ x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}} $$  

2. **标准化 (Standardization)**:  
	将数据调整为均值为0、标准差为1的分布，公式为：  
   
   $$ x_{std} = \frac{x - \mu}{\sigma} $$  

### (三) 线性回归模型可能对特征的尺度敏感的原因  

1. **解析解稳定性**  
	参数解析解 $w = (X^T X)^{-1} X^T y$ 中，若特征尺度差异大，矩阵 $X^T X$ 的条件数增大，导致求逆不稳定。  

2. **梯度下降效率**  
	不同尺度的特征需不同的学习率，尺度差异大会导致收敛速度慢。  

---

## 二、对于线性回归目标函数 $J(w) = \sum_{i=1}^N (w^{T} x_i - y_i)^2$，推导给出参数w的解析解形式，并思考对于实验所使用的数据集而言，采用标准方程组法求解参数w相较于梯度下降法有何优势或劣势。  

### （一）目标函数与解析解推导  

1. **目标函数**:  

   $$ J(w) = \sum_{i=1}^N (w^T x_i - y_i)^2 $$  

2. **解析解推导**:  

矩阵形式：$J(w) = (X w - y)^T (X w - y)$  

对w求导并令导数为零：  

   $$ \frac{\partial J(w)}{\partial w} = 2 X^T (X w - y) = 0 $$  
解得：  

   $$ w = (X^T X)^{-1} X^T y $$

### （二）标准方程组法相较于梯度下降法的优劣  

1. **优势**:  
   - 直接得到全局最优解，无需迭代调参。  
   - 当特征数d较小时，计算复杂度 $O(d^3)$ 可接受。  

2. **劣势**:  
   - 计算复杂度高，过高时矩阵求逆计算不可行。  
   - 存储需要 $O(d^2)$ 内存较大。  

---

## 三、实验中使用的评估指标（如均方误差MSE、均方根误差RMSE、决定系数$R^2$）分别反映了模型的哪些性能？如果某次实验的$R^2$值为负，可能是什么原因导致的？  

### （一）指标定义与意义  

1. **均方误差 (MSE)**:  
	反映预测值与真实值的平均平方误差，值越小越好。  
   $$ \text{MSE} = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y_i})^2 $$  

2. **均方根误差 (RMSE)**:  
	与目标变量单位一致，更易解释误差的实际影响。  
   $$ \text{RMSE} = \sqrt{\text{MSE}} $$  

3. **决定系数 ($R^2$)**:  
	表示模型解释的方差比例，范围 $(-\infty, 1]$，越接近1越好。  

### （二）$R^2$为负的原因  

$$ R^2 = 1 - \frac{\sum_{i=1}^N (y_i - \bar{y_l})^2}{\sum_{i=1}^N (y_i - \bar{y})^2} $$  
- **模型性能极差**：当 $\sum (y_i - \hat{y_l})^2 > \sum (y_i - \bar{y})^2$，即模型预测比直接取均值更差。  
- **可能原因**:  
	1. 数据存在严重噪声或非线性关系未被捕捉。  
	2. 特征与目标变量无关，模型欠拟合。  

---

## 四、在实验中，如果原始数据中存在非线性关系（如特征与目标变量呈二次函数关系），直接使用线性回归会导致模型性能不佳，思考通过何种方式能够更好的拟合特征与目标变量之间的关系。  

如果原始数据中存在非线性关系，直接使用线性回归无法捕捉特征与目标变量之间的非线性关系，会导致预测性能下降。  

### 改进方法  

1. **多项式特征扩展**：添加特征的高次项（如$x^2, x^3$）或交互项（如$x_1 x_2$）。  
2. **核方法**：将特征映射到高维空间，间接实现非线性拟合。  
3. **树模型**（如决策树、随机森林）：直接处理非线性关系，牺牲模型可解释性。  

**实验中的应用**：在步骤二中引入`sklearn`的`PolynomialFeatures`生成多项式特征，再使用线性回归拟合。  

---

## 五、你对本次实验课程内容、课程形式、实践平台使用等方面有哪些意见及改进建议？  

- 实践平台的使用不流畅，且无自动补全，最后使用本地IDE才完成。  